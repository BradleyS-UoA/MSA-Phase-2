{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":77309,"databundleVersionId":8644029,"sourceType":"competition"},{"sourceId":82618,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":69397,"modelId":94537},{"sourceId":82808,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":69559,"modelId":94696},{"sourceId":83288,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":69958,"modelId":95082}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Summary:\n\nIn order to load the images from the folders, two classes of custom image datasets are defined, one including labels and one for the test case. These classes is able to read from the directory and return the image (and label if training)\n\nAnother class called TransformedDataset is defined. This takes in the training dataset and apply a transformation to the images. \n\nAfter inspecting the images, I saw that the training dataset seems to be washed out, where the minimum brightness is set too high. This is mitigated by normalization of the images as part of its transformation.\n\nA 90%-10% training-test split is used to use as many images for training as possible.\n\nFor the training dataset, RandomHorizontalFlip and TrivialAugmentWide is used to change the training images. This is done to create new images the model has never seen, which is good for training as it can avoid overfitting of the model to the training dataset.\n\nThe datasets are loaded into dataloaders with a batch size of 64, meaning 64 images are passed before updating the gradient in the next set. This dataloader applys the transformations as needed.\n\nThe model selected is a Convolutional Neural Network, with 4 convolutional blocks and 1 fully connected block. Each convolutional block contains two convolutional layers before being pooled. The structure of our model is based on the paper \"An Introduction to Convolutional Neural Networks\" by Keiron O'Shea, Ryan Nash.\n\nA dropout rate of 0.2 for most layers and 0.6 for the last convolutional block is used. This is done to promote better generalization of the model, and avoid overfitting the model by remembering the images.\n\nThe CrossEntropyLoss loss function is used. For the optimizer, a SGD optimizer with momentum set at 0.2 is used. \n\nWith a learning rate of 0.01 initially and 0.001 afterwards, 500 + 100 epochs (500 on learning rate = 0.01 for training the initial checkpoint) is ran for the model.\n\nFor each epoch, the model trains by observing the loss function after each batch, and applies SGD optimizer to find the gradients, which is used for updating its weights.\nAfter training is done, it evaluates itself by applying the validation set on the model without calculating loss or updating weights, and returns the average loss and the accuracy percentage.\n\nAn accuracy rate of 92% on the validation set is observed. \n\nThe model is then used on the test set, and its output is written as submission.csv\n\n\n","metadata":{}},{"cell_type":"markdown","source":"To improve the model, one possible change would be implementing a residual function that can pass information from earlier blocks onto later blocks. This means that the last layer will not be limited to only the highly abstracted representation of the image.\n\nAnother way is to try out different optimizers and loss functions, as well as adjusting hyperparameters such as learning rate, momentum, and change parameters such as kernel size, dimension of convolutional layers etc.\n\nRunning more epochs at the current settings yielded no noticable results, but slight improvement is possible after more training.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T10:09:44.450821Z","iopub.execute_input":"2024-08-04T10:09:44.451188Z","iopub.status.idle":"2024-08-04T10:09:44.456349Z","shell.execute_reply.started":"2024-08-04T10:09:44.451158Z","shell.execute_reply":"2024-08-04T10:09:44.455479Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport timeit\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.io import read_image, ImageReadMode\nimport matplotlib.pyplot as plt\n#imported packages\n\n#Created classes for different types of data\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, f\"image_{idx}.png\")\n        image = read_image(img_path,mode=ImageReadMode.RGB)\n        label = self.img_labels.iloc[idx, 1]\n        return image, label\n\nclass TestDataset(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return 5000\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, f\"image_{idx}.png\")\n        image = read_image(img_path,mode=ImageReadMode.RGB)\n        if self.transform:\n            image = self.transform(image)\n            \n        return image\n\n#Created class for data that is transformed\nclass TransformedDataset(Dataset):\n    def __init__(self, dataset, transform):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        image = self.transform(image)\n        return image, label\n\n\n#After inspecting training dataset, minimum brightnes is too high\nmean, std = [0.495, 0.495, 0.495], [0.5, 0.5, 0.5]\n\n#Apply image augmentation \ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize(mean,std),\n    transforms.ToPILImage(),\n    transforms.TrivialAugmentWide(fill=0),\n    transforms.ToTensor(),\n])\n\nval_transform = transforms.Compose([\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize(mean,std),\n])\n\n\nfull_dataset = CustomImageDataset(annotations_file='/kaggle/input/nzmsa-2024/train.csv',img_dir='/kaggle/input/nzmsa-2024/cifar10_images/train/')\n\n\ntrain_size = int(0.9 * len(full_dataset))\nval_size = len(full_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\ntrain_dataset = TransformedDataset(train_dataset,train_transform)\nval_dataset = TransformedDataset(val_dataset,val_transform )\n#print(val_dataset[0][0].numpy().min(axis=1).min(axis=1))\n\n#print(train_dataset[1][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:08:11.700975Z","iopub.execute_input":"2024-07-31T11:08:11.701434Z","iopub.status.idle":"2024-07-31T11:08:11.738964Z","shell.execute_reply.started":"2024-07-31T11:08:11.701391Z","shell.execute_reply":"2024-07-31T11:08:11.738045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspecting the training dataset shows that the minimum value for the training images do not start from 0, instead when images converted to float, minimum value is around 0.5\nThe normalize function fixes this by removing ~0.5 from all images then scaling them to be between 0-1.\nToPILImage is required to apply trivial augment.\n\nUses 90% of the labeled dataset as training, 10% for validaiton.","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nnum_workers = torch.cuda.device_count() * 4\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              num_workers=num_workers, pin_memory=True,\n                              batch_size=64,\n                              shuffle=True)\nval_dataloader = DataLoader(dataset=val_dataset,\n                            num_workers=num_workers, pin_memory=True,\n                            batch_size=64,\n                            shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:08:15.306236Z","iopub.execute_input":"2024-07-31T11:08:15.307012Z","iopub.status.idle":"2024-07-31T11:08:15.313032Z","shell.execute_reply.started":"2024-07-31T11:08:15.306982Z","shell.execute_reply":"2024-07-31T11:08:15.312099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uses the dataloader to select the images from the dataset.","metadata":{}},{"cell_type":"code","source":"def show_data(img):\n    plt.imshow(img[0].permute(1,2,0))\n    plt.title('y = '+ str(img[1]))\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:08:16.683908Z","iopub.execute_input":"2024-07-31T11:08:16.684681Z","iopub.status.idle":"2024-07-31T11:08:16.689144Z","shell.execute_reply.started":"2024-07-31T11:08:16.684649Z","shell.execute_reply":"2024-07-31T11:08:16.688221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_data(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:08:30.853540Z","iopub.execute_input":"2024-07-31T11:08:30.854131Z","iopub.status.idle":"2024-07-31T11:08:31.039705Z","shell.execute_reply.started":"2024-07-31T11:08:30.854101Z","shell.execute_reply":"2024-07-31T11:08:31.038938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test function to see what the model sees","metadata":{}},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        self.model = nn.Sequential(\n            #Conv 1\n            nn.Conv2d(3, 128, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            #Normalize for 2nd pass\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            #nn.MaxPool2d(kernel_size=2, stride=2), try not to pool initial layers\n            #Dropout some connections\n            nn.Dropout(0.2),\n\n            #Conv 2\n            nn.Conv2d(128, 256, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.2),\n\n            #Conv 3\n            nn.Conv2d(256, 512, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.2),\n            \n            #Conv 4\n            nn.Conv2d(512, 1024, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(1024),\n            nn.Conv2d(1024, 1024, kernel_size=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(1024),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.6),\n\n            #FC\n            nn.Flatten(),\n            nn.Linear(36864, 768),\n            nn.ReLU(inplace=True),\n            nn.Linear(768, 10),\n            #Turn into percentage\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:08:32.956519Z","iopub.execute_input":"2024-07-31T11:08:32.956867Z","iopub.status.idle":"2024-07-31T11:08:32.969184Z","shell.execute_reply.started":"2024-07-31T11:08:32.956838Z","shell.execute_reply":"2024-07-31T11:08:32.968336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4 Convolution blocks and 1 fully connected block. High dropout is used to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"#Initialize settings\nepochs = 100\nmodel1 =  CNNModel().to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.001\n#Uses SGD with small momentum for hopefully better search\noptimizer = SGD(model1.parameters(), lr = learning_rate, momentum = 0.2)\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\n#Load checkpoint from before if necessary\nmodel1.load_state_dict(torch.load('/kaggle/input/v3/pytorch/default/1/CNN.pt'))\nstart = timeit.default_timer()\nfor epoch in tqdm(range(epochs), position=0, leave=True):\n    #Start training\n    model1.train()\n    \n    #Reset labels and predictions and loss\n    train_labels = []\n    train_preds = []\n    train_running_loss = 0\n    #Loop for each image in a batch\n    for idx, (x, y) in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n        img = x.float().to(device)\n        label = y.type(torch.uint8).to(device)\n        #Reset the gradient from before\n        optimizer.zero_grad()\n        #Find prediciton and record\n        pred = model1(img)\n        pred_label = torch.argmax(pred, dim=1)\n        train_labels.extend(label)\n        train_preds.extend(pred_label)\n        #Find loss of prediction\n        loss = criterion(pred, label)\n        train_running_loss += loss.item()\n        loss.backward() #Back propagation\n        optimizer.step() #Gradient descent\n    \n    #Find average loss\n    train_loss = train_running_loss / (idx + 1)\n    \n    #Find accuracy by adding 1 if prediction match label, and divide by all labels\n    train_accuracy = sum(1 for x,y in zip(train_preds, train_labels) if x==y) / len(train_labels)\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n    \n\n    #Evaluate model, no more training\n    model1.eval()\n    \n    #Similar to train except no changes to gradient\n    val_labels = []\n    val_preds = []\n    val_running_loss = 0\n    with torch.no_grad():\n        for idx, (x, y) in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n            img = x.float().to(device)\n            label = y.type(torch.uint8).to(device)\n            pred = model1(img)\n            pred_label = torch.argmax(pred, dim=1)\n            val_labels.extend(label)\n            val_preds.extend(pred_label)\n            loss = criterion(pred, label)\n            val_running_loss += loss.item()\n\n    val_loss = val_running_loss / (idx + 1)\n    \n    val_accuracy = sum(1 for x,y in zip(val_preds, val_labels) if x==y) / len(val_labels)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n    \n    #Print status\n    print(f\"EPOCH {epoch+1}\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Valid Loss: {val_loss:.4f}\")\n    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Valid Accuracy: {val_accuracy:.4f}\")\n\n#Print when stopped training\nstop= timeit.default_timer()\nprint(f\"Training Time: {stop-start:.2f} s\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:12:40.833365Z","iopub.execute_input":"2024-07-31T11:12:40.834329Z","iopub.status.idle":"2024-07-31T11:14:15.362975Z","shell.execute_reply.started":"2024-07-31T11:12:40.834285Z","shell.execute_reply":"2024-07-31T11:14:15.361954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the model\ntorch.save(model1.state_dict(), '/kaggle/working/CNN.pt')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:14:30.423515Z","iopub.execute_input":"2024-07-31T11:14:30.423928Z","iopub.status.idle":"2024-07-31T11:14:30.713297Z","shell.execute_reply.started":"2024-07-31T11:14:30.423891Z","shell.execute_reply":"2024-07-31T11:14:30.712242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict on the test dataset\ntest_dataset = TestDataset(img_dir='/kaggle/input/nzmsa-2024/cifar10_images/test',transform=transforms.ConvertImageDtype(torch.float))\nsubmission = pd.read_csv('/kaggle/input/nzmsa-2024/sample_submission.csv')\n\n\n#Load the test data\ntest_dataloader = DataLoader(dataset=test_dataset)\n\n\nmodel =  model1\n#model.load_state_dict(torch.load('/kaggle/input/1/pytorch/default/1/CNN.pt'))\n\n#Run on evaluation mode\nmodel.eval()\n\n#Do not change gradients\nwith torch.no_grad():\n    #Loop for every test image\n    for idx, x in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n                img = x.float().to(device)\n                pred = model(img)\n                pred_label = torch.argmax(pred, dim=1)\n                #Write results to submission dataframe\n                submission.loc[idx,'id'] = idx\n                submission.loc[idx,'label'] = pred_label.item()\n#Convert to int\nsubmission['label'] = submission['label'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:14:32.457625Z","iopub.execute_input":"2024-07-31T11:14:32.458295Z","iopub.status.idle":"2024-07-31T11:19:33.341516Z","shell.execute_reply.started":"2024-07-31T11:14:32.458265Z","shell.execute_reply":"2024-07-31T11:19:33.340559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test function to see if it works\ndef show_test(id):\n    plt.imshow(test_dataset[id].permute(1,2,0))\n    plt.title('y = '+ str(submission.loc[id,'label']))\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:19:49.897960Z","iopub.execute_input":"2024-07-31T11:19:49.898351Z","iopub.status.idle":"2024-07-31T11:19:49.904393Z","shell.execute_reply.started":"2024-07-31T11:19:49.898320Z","shell.execute_reply":"2024-07-31T11:19:49.903341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    show_test(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:20:21.916735Z","iopub.execute_input":"2024-07-31T11:20:21.917502Z","iopub.status.idle":"2024-07-31T11:20:24.059500Z","shell.execute_reply.started":"2024-07-31T11:20:21.917461Z","shell.execute_reply":"2024-07-31T11:20:24.058580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write to submission\nsubmission.to_csv('submission.csv',index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:19:52.061591Z","iopub.execute_input":"2024-07-31T11:19:52.061956Z","iopub.status.idle":"2024-07-31T11:19:52.086972Z","shell.execute_reply.started":"2024-07-31T11:19:52.061926Z","shell.execute_reply":"2024-07-31T11:19:52.086042Z"},"trusted":true},"execution_count":null,"outputs":[]}]}